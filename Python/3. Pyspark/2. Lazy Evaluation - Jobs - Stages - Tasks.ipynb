{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1158092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Point PySpark to the exact Python you're running right now ---\n",
    "import os, sys\n",
    "\n",
    "py = sys.executable  # e.g., C:\\Users\\PX\\anaconda3\\envs\\music-chatbot\\python.exe\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = py\n",
    "os.environ[\"PYSPARK_PYTHON\"] = py\n",
    "\n",
    "# --- 2) Stop any existing Spark session cleanly (if already created) ---\n",
    "try:\n",
    "    spark.stop()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"PySpark-Windows-Fix\")\n",
    "    # Ensures executors pick the same Python as the driver\n",
    "    .config(\"spark.pyspark.driver.python\", py)\n",
    "    .config(\"spark.pyspark.python\", py)\n",
    "    # Optional: Arrow speeds up Pandas ‚Üî Spark conversions if available\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d654a",
   "metadata": {},
   "source": [
    "# üí§ Lazy Evaluation in Spark\n",
    "\n",
    "**Definition:**  \n",
    "Spark does not execute transformations immediately.  \n",
    "Instead, it **builds a logical plan (DAG)** of all transformations and only executes them when an **action** is called.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† How it works:\n",
    "1. **Transformations** (e.g., `select`, `filter`, `map`, `withColumn`)\n",
    "   - **Lazy:** They just *record* the steps in a DAG.\n",
    "   - No actual computation or data movement happens yet.\n",
    "\n",
    "2. **Actions** (e.g., `show`, `count`, `collect`, `write`)\n",
    "   - **Trigger execution:** Spark optimizes the DAG, splits it into stages, schedules tasks, and computes results.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Benefits of Lazy Evaluation\n",
    "- **Optimization:** Spark can reorder operations, combine filters, and minimize shuffles before running.\n",
    "- **Efficiency:** Avoids doing unnecessary work (e.g., if you filter then drop columns, Spark will push the drop down first).\n",
    "- **Fault tolerance:** DAG keeps track of lineage ‚Üí recomputes lost partitions only when needed.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Typical Interview Questions\n",
    "- What is lazy evaluation in Spark?\n",
    "- Give an example of a transformation and an action.\n",
    "- Why does Spark use lazy evaluation?\n",
    "- How does lazy evaluation improve performance?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7c5dd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Transformations defined, but no computation has run yet!\n",
      "+---+----------+\n",
      "| id|double_val|\n",
      "+---+----------+\n",
      "|  2|         4|\n",
      "|  4|         8|\n",
      "|  6|        12|\n",
      "|  8|        16|\n",
      "+---+----------+\n",
      "\n",
      "‚è±Ô∏è Action triggered execution; took 1.5328 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "# Create a DataFrame\n",
    "df_lazy = spark.range(1, 10)\n",
    "\n",
    "# Define transformations (LAZY, nothing runs yet)\n",
    "df_transformed = df_lazy.filter(df_lazy.id % 2 == 0) \\\n",
    "                        .withColumn(\"double_val\", df_lazy.id * 2)\n",
    "\n",
    "print(\"‚úÖ Transformations defined, but no computation has run yet!\")\n",
    "\n",
    "# Trigger an action (ACTUAL EXECUTION happens here)\n",
    "start = time.time()\n",
    "df_transformed.show()\n",
    "end = time.time()\n",
    "\n",
    "print(f\"‚è±Ô∏è Action triggered execution; took {end - start:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2197959",
   "metadata": {},
   "source": [
    "# üéØ Lazy Evaluation ‚Äì Interview Answers\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùì What is lazy evaluation in Spark?\n",
    "üí° Lazy evaluation means Spark does **not execute transformations immediately**.  \n",
    "Instead, it builds a **logical plan (DAG)** of all transformations and only executes it when an **action** (like `show`, `count`, `collect`, `write`) is called.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùì Give an example of a transformation and an action\n",
    "- **Transformation (lazy):** Only adds a step to the DAG (no computation yet).  \n",
    "- **Action:** Triggers Spark to optimize the DAG, run tasks, and produce results.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùì Why does Spark use lazy evaluation?\n",
    "- üß† **Global optimization** ‚Äì Spark can optimize the whole job before running it.  \n",
    "- üõë **Avoids unnecessary work** ‚Äì skips computations for unused transformations.  \n",
    "- üîÑ **Fault tolerance** ‚Äì DAG lineage lets Spark recompute only missing data if a node fails.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùì How does lazy evaluation improve performance?\n",
    "- üîó **Pipelining:** Combines multiple transformations into fewer stages.  \n",
    "- üéØ **Predicate pushdown:** Filters early to shrink data before shuffles.  \n",
    "- üì¶ **Minimized shuffles:** Reduces costly network transfers.  \n",
    "- ‚ö° **Efficiency:** Computes only what‚Äôs required, when it‚Äôs required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2437143d",
   "metadata": {},
   "source": [
    "# üîÄ Narrow vs Wide Transformations in Spark\n",
    "\n",
    "Understanding narrow vs wide transformations is crucial for interviews ‚Äî it explains how Spark decides **when to create a new stage** in the DAG.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Narrow Transformations\n",
    "- Each input partition contributes to **exactly one output partition**.\n",
    "- No data needs to move across the cluster.\n",
    "- **No shuffle** occurs.\n",
    "- Examples:\n",
    "  - `map()`, `filter()`, `select()`, `withColumn()`, `union()`, `sample()`\n",
    "\n",
    "**Effect:** These operations can be **pipelined** together in the same stage ‚Üí faster execution.\n",
    "\n",
    "---\n",
    "\n",
    "## üî∏ Wide Transformations\n",
    "- Input partitions contribute to **multiple output partitions**.\n",
    "- **Data must be shuffled** across the network (expensive).\n",
    "- Examples:\n",
    "  - `groupBy()`, `reduceByKey()`, `join()`, `distinct()`, `orderBy()`\n",
    "\n",
    "**Effect:** Spark creates a **new stage** after a wide transformation because data has to be redistributed across executors.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Why This Matters\n",
    "- Wide transformations are **more expensive** (network + disk I/O).\n",
    "- Optimizing pipelines means:\n",
    "  - Filter early (reduce data before shuffle).\n",
    "  - Repartition intelligently to control shuffle size.\n",
    "  - Cache if needed to avoid recomputation after expensive shuffles.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Typical Interview Questions\n",
    "- What is the difference between narrow and wide transformations?\n",
    "- Which type causes a shuffle and why?\n",
    "- Give examples of narrow and wide transformations.\n",
    "- Why does Spark create a new stage after a wide transformation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7be56849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Narrow transformations output ===\n",
      "+---+----------+\n",
      "| id|double_val|\n",
      "+---+----------+\n",
      "|  0|         0|\n",
      "|  2|         4|\n",
      "|  4|         8|\n",
      "|  6|        12|\n",
      "|  8|        16|\n",
      "| 10|        20|\n",
      "| 12|        24|\n",
      "| 14|        28|\n",
      "| 16|        32|\n",
      "| 18|        36|\n",
      "+---+----------+\n",
      "\n",
      "\n",
      "=== Wide transformation output (shuffle happens here) ===\n",
      "+----------+-----+\n",
      "|double_val|count|\n",
      "+----------+-----+\n",
      "|         0|    1|\n",
      "|         8|    1|\n",
      "|         4|    1|\n",
      "|        12|    1|\n",
      "|        16|    1|\n",
      "|        20|    1|\n",
      "|        28|    1|\n",
      "|        24|    1|\n",
      "|        32|    1|\n",
      "|        36|    1|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Sample DataFrame\n",
    "df_demo = spark.range(0, 20)\n",
    "\n",
    "# Narrow transformations (no shuffle)\n",
    "df_narrow = df_demo.filter(df_demo.id % 2 == 0) \\\n",
    "                   .withColumn(\"double_val\", df_demo.id * 2)\n",
    "\n",
    "# Wide transformation (shuffle required)\n",
    "df_wide = df_narrow.groupBy(\"double_val\").count()\n",
    "\n",
    "# Trigger execution\n",
    "print(\"=== Narrow transformations output ===\")\n",
    "df_narrow.show()\n",
    "\n",
    "print(\"\\n=== Wide transformation output (shuffle happens here) ===\")\n",
    "df_wide.show()\n",
    "\n",
    "# Tip: You can open the Spark UI (driver host printed earlier) to see\n",
    "# the DAG visualization and confirm that groupBy caused a shuffle (new stage).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ae9667",
   "metadata": {},
   "source": [
    "# üß© Jobs, Stages, and Tasks in Spark\n",
    "\n",
    "Understanding **jobs ‚Üí stages ‚Üí tasks** is key to explaining Spark's execution model.\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Execution Hierarchy\n",
    "\n",
    "1Ô∏è‚É£ **Job**\n",
    "- A job is triggered by an **action** (e.g., `count()`, `collect()`, `show()`).\n",
    "- One Spark application can have many jobs.\n",
    "- Example: Calling `.count()` triggers a job.\n",
    "\n",
    "2Ô∏è‚É£ **Stage**\n",
    "- A job is divided into **stages** based on **shuffle boundaries**.\n",
    "- Each stage contains **narrow transformations** that can be pipelined together.\n",
    "- New stage starts when Spark needs to **shuffle data** (e.g., `groupBy`, `join`).\n",
    "\n",
    "3Ô∏è‚É£ **Task**\n",
    "- The smallest unit of work in Spark.\n",
    "- Each task runs on **one partition** of the data.\n",
    "- Number of tasks = number of partitions in the stage.\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Relationship\n",
    "\n",
    "**Action ‚ûú Job ‚ûú Stages ‚ûú Tasks**\n",
    "\n",
    "Example flow for a `groupBy`:\n",
    "- `count()` ‚Üí creates **1 Job**\n",
    "- Spark breaks it into **2 Stages** (before and after shuffle)\n",
    "- Each stage runs multiple **Tasks** in parallel (one per partition)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64136d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|key|cnt|\n",
      "+---+---+\n",
      "|  0| 10|\n",
      "|  7| 10|\n",
      "|  6| 10|\n",
      "|  9| 10|\n",
      "|  5| 10|\n",
      "|  1| 10|\n",
      "|  3| 10|\n",
      "|  8| 10|\n",
      "|  2| 10|\n",
      "|  4| 10|\n",
      "+---+---+\n",
      "\n",
      "\n",
      "‚úÖ Check Spark UI (driver host printed earlier) ‚Üí you'll see:\n",
      "- 1 job created for the show() action\n",
      "- 2 stages (before and after shuffle)\n",
      "- Multiple tasks per stage (1 per partition)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Sample DataFrame\n",
    "df_jobs = spark.range(0, 100).withColumn(\"key\", F.col(\"id\") % 10)\n",
    "\n",
    "# Trigger an action with a shuffle (groupBy causes a wide transformation)\n",
    "grouped = df_jobs.groupBy(\"key\").agg(F.count(\"*\").alias(\"cnt\"))\n",
    "\n",
    "# Action: triggers a job with multiple stages & tasks\n",
    "grouped.show()\n",
    "\n",
    "print(\"\\n‚úÖ Check Spark UI (driver host printed earlier) ‚Üí you'll see:\")\n",
    "print(\"- 1 job created for the show() action\")\n",
    "print(\"- 2 stages (before and after shuffle)\")\n",
    "print(\"- Multiple tasks per stage (1 per partition)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
