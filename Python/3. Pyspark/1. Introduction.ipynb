{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ab7b69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Point PySpark to the exact Python you're running right now ---\n",
    "import os, sys\n",
    "\n",
    "py = sys.executable  # e.g., C:\\Users\\PX\\anaconda3\\envs\\music-chatbot\\python.exe\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = py\n",
    "os.environ[\"PYSPARK_PYTHON\"] = py\n",
    "\n",
    "# --- 2) Stop any existing Spark session cleanly (if already created) ---\n",
    "try:\n",
    "    spark.stop()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"PySpark-Windows-Fix\")\n",
    "    # Ensures executors pick the same Python as the driver\n",
    "    .config(\"spark.pyspark.driver.python\", py)\n",
    "    .config(\"spark.pyspark.python\", py)\n",
    "    # Optional: Arrow speeds up Pandas ‚Üî Spark conversions if available\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d37fad",
   "metadata": {},
   "source": [
    "# üî• What is Apache Spark?\n",
    "\n",
    "Apache **Spark** is an **open-source, distributed computing framework** designed for **big data processing** and **analytics**.  \n",
    "It allows you to process **large datasets** in a **cluster of machines** (or locally) using **parallel computing** ‚Äî much faster than traditional tools like Hadoop MapReduce.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° Key Features\n",
    "1. **Speed**\n",
    "   - Uses in-memory computation (RDD caching) ‚Üí much faster than Hadoop MapReduce (up to 100x).\n",
    "2. **Ease of Use**\n",
    "   - APIs available in Python (PySpark), Scala, Java, and SQL.\n",
    "3. **Versatility**\n",
    "   - Handles **batch processing**, **streaming**, **machine learning** (MLlib), and **graph processing** (GraphX).\n",
    "4. **Scalability**\n",
    "   - Runs on a laptop, a cluster of machines, or in the cloud (AWS EMR, Databricks, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "## üß† High-Level Architecture\n",
    "Spark has **two main parts**:\n",
    "\n",
    "- **Driver Program**\n",
    "  - The \"brain\" of Spark.\n",
    "  - Creates the **SparkSession**, builds the DAG (Directed Acyclic Graph), and coordinates tasks.\n",
    "\n",
    "- **Cluster Manager + Executors**\n",
    "  - Cluster Manager assigns resources (CPU, memory).\n",
    "  - Executors run the actual tasks on worker nodes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16182505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|     Name|Age|\n",
      "+---------+---+\n",
      "|    Alice| 29|\n",
      "|      Bob| 34|\n",
      "|Catherine| 25|\n",
      "+---------+---+\n",
      "\n",
      "\n",
      "Interpreter: c:\\Users\\PX\\anaconda3\\envs\\music-chatbot\\python.exe\n",
      "Spark version: 4.0.1\n",
      "PySpark version: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Alice\", 29), (\"Bob\", 34), (\"Catherine\", 25)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "df.show()\n",
    "\n",
    "print(\"\\nInterpreter:\", py)\n",
    "print(\"Spark version:\", spark.version)\n",
    "import pyspark\n",
    "print(\"PySpark version:\", pyspark.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cbd257",
   "metadata": {},
   "source": [
    "# üèóÔ∏è Spark Architecture (Driver, Executors, DAG)\n",
    "\n",
    "Spark follows a **master/worker architecture**.  \n",
    "Understanding this is crucial for interviews ‚Äî it shows you know **how Spark runs your code under the hood**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Key Components\n",
    "\n",
    "### 1Ô∏è‚É£ **Driver Program**\n",
    "- The \"brain\" of a Spark application.\n",
    "- Runs in your process (where you call `SparkSession.builder`).\n",
    "- Responsibilities:\n",
    "  - Creates `SparkContext` (or `SparkSession` in modern API).\n",
    "  - Converts your code into a **DAG** (Directed Acyclic Graph).\n",
    "  - Splits the DAG into **stages**.\n",
    "  - Schedules tasks to run on **executors**.\n",
    "  - Collects results and sends them back to you.\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ **Cluster Manager**\n",
    "- Allocates resources (CPU, memory) to Spark.\n",
    "- Can be:\n",
    "  - **Standalone** (built-in Spark manager)\n",
    "  - **YARN** (Hadoop cluster manager)\n",
    "  - **Mesos** or **Kubernetes**\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ **Executors**\n",
    "- Worker processes running on cluster nodes.\n",
    "- Each executor:\n",
    "  - Runs **tasks** assigned by the driver.\n",
    "  - Stores **cached data** in memory.\n",
    "- Executors live for the lifetime of your application.\n",
    "\n",
    "---\n",
    "\n",
    "## üó∫Ô∏è DAG, Stages, and Tasks\n",
    "\n",
    "1. **DAG (Directed Acyclic Graph)**\n",
    "   - Logical plan of all transformations you wrote (`map`, `filter`, `select`).\n",
    "   - Shows *what to do*, not how.\n",
    "\n",
    "2. **Stages**\n",
    "   - DAG is split into **stages** at shuffle boundaries (when data needs to move across the cluster).\n",
    "   - Each stage is a set of parallel tasks.\n",
    "\n",
    "3. **Tasks**\n",
    "   - Smallest unit of work in Spark.\n",
    "   - Each task is sent to an executor core.\n",
    "   - Runs on a **partition** of the data.\n",
    "\n",
    "---\n",
    "\n",
    "## üñºÔ∏è Visual Overview\n",
    "\n",
    "Driver ‚ûú Cluster Manager ‚ûú Executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f60fea50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Spark Configuration ===\n",
      "spark.rdd.compress = True\n",
      "spark.hadoop.fs.s3a.vectored.read.min.seek.size = 128K\n",
      "spark.pyspark.python = c:\\Users\\PX\\anaconda3\\envs\\music-chatbot\\python.exe\n",
      "spark.pyspark.driver.python = c:\\Users\\PX\\anaconda3\\envs\\music-chatbot\\python.exe\n",
      "spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true\n",
      "spark.sql.artifact.isolation.enabled = false\n",
      "spark.master = local[*]\n",
      "spark.sql.execution.arrow.pyspark.enabled = true\n",
      "spark.sql.warehouse.dir = file:/C:/Users/PX/Desktop/java-python-ipynb-practice/Python/3.%20Pyspark/spark-warehouse\n",
      "spark.driver.host = DESKTOP-7JTCESD\n",
      "spark.executor.id = driver\n",
      "spark.submit.pyFiles = \n",
      "spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true\n",
      "spark.hadoop.fs.s3a.vectored.read.max.merged.size = 2M\n",
      "spark.app.startTime = 1758035389320\n",
      "spark.submit.deployMode = client\n",
      "spark.app.id = local-1758035389379\n",
      "spark.app.submitTime = 1758035222198\n",
      "spark.serializer.objectStreamReset = 100\n",
      "spark.driver.port = 63057\n",
      "spark.app.name = PySpark-Windows-Fix\n",
      "spark.ui.showConsoleProgress = true\n",
      "\n",
      "Driver host: http://DESKTOP-7JTCESD:4040\n",
      "Master: local[*]\n",
      "Application ID: local-1758035389379\n",
      "Default parallelism (tasks per stage): 8\n"
     ]
    }
   ],
   "source": [
    "# Inspect Spark environment to see driver, executor configs, and cluster manager info\n",
    "print(\"=== Spark Configuration ===\")\n",
    "for k, v in spark.sparkContext.getConf().getAll():\n",
    "    print(f\"{k} = {v}\")\n",
    "\n",
    "print(\"\\nDriver host:\", spark.sparkContext.uiWebUrl or \"No UI available\")\n",
    "print(\"Master:\", spark.sparkContext.master)\n",
    "print(\"Application ID:\", spark.sparkContext.applicationId)\n",
    "print(\"Default parallelism (tasks per stage):\", spark.sparkContext.defaultParallelism)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82db4da5",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Spark Configuration (Explained)\n",
    "\n",
    "Below is a quick explanation of the key properties you saw:\n",
    "\n",
    "- **spark.rdd.compress** ‚Üí Enables compression of serialized RDD partitions (saves memory, improves performance).\n",
    "- **spark.hadoop.fs.s3a.vectored.read.min.seek.size** ‚Üí Minimum seek size for vectored reads from S3 (optimizes remote reads).\n",
    "- **spark.pyspark.python** ‚Üí Path to Python executable used by executors.\n",
    "- **spark.pyspark.driver.python** ‚Üí Path to Python executable used by driver program.\n",
    "- **spark.executor.extraJavaOptions** ‚Üí Extra JVM options passed to executor processes (e.g., module opens, system properties).\n",
    "- **spark.sql.artifact.isolation.enabled** ‚Üí Whether to isolate SQL artifacts per session (false = shared globally).\n",
    "- **spark.master** ‚Üí Cluster master URL (`local[*]` means run locally using all available CPU cores).\n",
    "- **spark.sql.execution.arrow.pyspark.enabled** ‚Üí Enables Apache Arrow for faster Spark ‚Üî Pandas conversion.\n",
    "- **spark.sql.warehouse.dir** ‚Üí Default directory for Spark SQL managed tables (warehouse).\n",
    "- **spark.driver.host** ‚Üí Hostname of the machine running the driver program.\n",
    "- **spark.executor.id** ‚Üí ID of the executor running this session (`driver` means no cluster executors, local mode).\n",
    "- **spark.submit.pyFiles** ‚Üí List of `.zip`/`.egg`/`.py` files sent with job (empty here).\n",
    "- **spark.driver.extraJavaOptions** ‚Üí Extra JVM options passed to the driver process.\n",
    "- **spark.hadoop.fs.s3a.vectored.read.max.merged.size** ‚Üí Max size of merged S3 vectored reads (controls I/O efficiency).\n",
    "- **spark.app.startTime** ‚Üí Application start timestamp (epoch ms).\n",
    "- **spark.submit.deployMode** ‚Üí Deployment mode (`client` = driver runs locally, `cluster` = driver runs on cluster).\n",
    "- **spark.app.id** ‚Üí Unique application ID for this Spark job.\n",
    "- **spark.app.submitTime** ‚Üí Timestamp when the app was submitted.\n",
    "- **spark.serializer.objectStreamReset** ‚Üí Resets object output stream after this many objects (avoids memory leak).\n",
    "- **spark.driver.port** ‚Üí Port used by the driver to communicate with executors.\n",
    "- **spark.app.name** ‚Üí Name of the application (set in `SparkSession.builder.appName()`).\n",
    "- **spark.ui.showConsoleProgress** ‚Üí Whether to show progress bars in console for running stages.\n",
    "- **Driver host (UI)** ‚Üí URL for Spark Web UI (shows jobs, stages, storage).\n",
    "- **Master** ‚Üí The cluster master Spark is connected to (`local[*]` means no cluster, run locally).\n",
    "- **Application ID** ‚Üí Unique identifier of this running application (useful for logs).\n",
    "- **Default parallelism** ‚Üí Number of tasks per stage (usually equals number of cores available).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bfd3a4",
   "metadata": {},
   "source": [
    "# üöÄ Benefits of Apache Spark\n",
    "\n",
    "Apache Spark became the most popular big-data engine because of its **speed**, **ease of use**, and **versatility**.  \n",
    "Here are the key benefits you should know for interviews:\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° 1. Speed\n",
    "- **In-memory computation**: Avoids writing intermediate results to disk (unlike Hadoop MapReduce).\n",
    "- Optimized query engine with DAG execution ‚Üí up to **100x faster** than MapReduce for iterative workloads.\n",
    "\n",
    "---\n",
    "\n",
    "## üíª 2. Ease of Use\n",
    "- High-level APIs available in **Python (PySpark)**, Scala, Java, R, and SQL.\n",
    "- Provides a **unified interface** for batch processing, streaming, ML, and graph processing.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÄ 3. Unified Engine\n",
    "- Same framework handles:\n",
    "  - **Batch processing** (ETL, transformations)\n",
    "  - **Streaming** (near real-time data with Structured Streaming)\n",
    "  - **Machine Learning** (MLlib library)\n",
    "  - **Graph processing** (GraphX)\n",
    "\n",
    "---\n",
    "\n",
    "## üìà 4. Scalability\n",
    "- Runs on a laptop, a single server, or a large cluster.\n",
    "- Works with on-prem clusters or cloud platforms (AWS EMR, Databricks, GCP Dataproc, Azure Synapse).\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è 5. Fault Tolerance\n",
    "- Uses **RDD lineage** to recompute lost partitions automatically.\n",
    "- No need for manual recovery ‚Äî ensures reliability even if executors fail.\n",
    "\n",
    "---\n",
    "\n",
    "## üíæ 6. Multiple Data Source Support\n",
    "- Reads from **HDFS, S3, Azure Blob, GCS, Cassandra, Kafka, JDBC, JSON, Parquet, ORC**, etc.\n",
    "- Makes it easy to build pipelines across heterogeneous data systems.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8213cc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PySpark result: 142858\n",
      "‚è±Ô∏è PySpark execution time: 0.1981 seconds\n",
      "\n",
      "‚úÖ Pandas result: 142858\n",
      "‚è±Ô∏è Pandas execution time: 0.0190 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# --- PySpark Version ---\n",
    "df_spark = spark.range(0, 1_000_000)\n",
    "\n",
    "start_spark = time.time()\n",
    "spark_result = df_spark.filter(df_spark.id % 7 == 0).count()\n",
    "end_spark = time.time()\n",
    "\n",
    "print(f\"‚úÖ PySpark result: {spark_result}\")\n",
    "print(f\"‚è±Ô∏è PySpark execution time: {end_spark - start_spark:.4f} seconds\\n\")\n",
    "\n",
    "# --- Pandas Version ---\n",
    "df_pandas = pd.DataFrame({\"id\": range(0, 1_000_000)})\n",
    "\n",
    "start_pandas = time.time()\n",
    "pandas_result = (df_pandas[\"id\"] % 7 == 0).sum()\n",
    "end_pandas = time.time()\n",
    "\n",
    "print(f\"‚úÖ Pandas result: {pandas_result}\")\n",
    "print(f\"‚è±Ô∏è Pandas execution time: {end_pandas - start_pandas:.4f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7df299",
   "metadata": {},
   "source": [
    "# üêº Pandas vs üî• Spark: Why Pandas Can Be Faster Here\n",
    "\n",
    "In our test, **Pandas was faster** ‚Äî and this is actually normal for small data.  \n",
    "Here‚Äôs why:\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Spark Overhead\n",
    "- Spark has to:\n",
    "  - Spin up a JVM (Java Virtual Machine)\n",
    "  - Create a Spark driver + executor\n",
    "  - Serialize/deserialize data between Python and JVM\n",
    "- This startup overhead can take **hundreds of milliseconds**, which dominates the runtime for small datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## üêº Pandas Strength\n",
    "- Pandas runs in a **single Python process**, no cluster overhead.\n",
    "- For data that **fits in memory** (e.g., < 1‚Äì2 GB), Pandas is usually faster for simple operations.\n",
    "\n",
    "---\n",
    "\n",
    "## üî• When Spark Shines\n",
    "- When the dataset is **too big for memory** on one machine (10+ GB).\n",
    "- When you need to **distribute processing** across multiple cores/machines.\n",
    "- When you need to integrate with **distributed storage** (HDFS, S3, Azure Blob, etc.).\n",
    "- When running **complex pipelines** where Spark can optimize stages and cache intermediate results.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Interview Insight\n",
    "- **Do not say Spark is \"always faster.\"**  \n",
    "  A strong candidate will say:\n",
    "  > \"Spark is designed for distributed, large-scale data. For small, local datasets, Pandas is often faster due to lower overhead.\"\n",
    "\n",
    "This shows you understand **trade-offs** and choose the right tool for the job.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b8cc08",
   "metadata": {},
   "source": [
    "# üî• When PySpark is Faster (and Why)\n",
    "\n",
    "PySpark usually beats Pandas when:\n",
    "- Data is **large** (10M+ rows) or **doesn‚Äôt fit in RAM** on one machine.\n",
    "- Workloads involve **wide shuffles** (e.g., `groupBy`, `join`, `distinct`) that benefit from parallelism.\n",
    "- You can leverage **multiple cores** (or a cluster) to process partitions in parallel.\n",
    "\n",
    "Below, we benchmark a large **groupBy + join** on up to tens of millions of rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4c28377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark (deterministic, stable) top-5:\n",
      "   key     cnt       sum_val     avg_val  avg_weight\n",
      "0    0  100000  2.499975e+12  24999750.0         0.0\n",
      "1    1  100000  2.499975e+12  24999750.5         1.0\n",
      "2    2  100000  2.499975e+12  24999751.0         2.0\n",
      "3    3  100000  2.499975e+12  24999751.5         3.0\n",
      "4    4  100000  2.499975e+12  24999752.0         4.0\n",
      "‚è±Ô∏è Spark time: 1.67 sec\n",
      "\n",
      "‚úÖ Pandas (deterministic, stable) top-5:\n",
      "   key     cnt       sum_val     avg_val  avg_weight\n",
      "0    0  100000  2.499975e+12  24999750.0         0.0\n",
      "1    1  100000  2.499975e+12  24999750.5         1.0\n",
      "2    2  100000  2.499975e+12  24999751.0         2.0\n",
      "3    3  100000  2.499975e+12  24999751.5         3.0\n",
      "4    4  100000  2.499975e+12  24999752.0         4.0\n",
      "‚è±Ô∏è Pandas time: 17.67 sec\n",
      "\n",
      "Columns equal (keys, cnt): True\n"
     ]
    }
   ],
   "source": [
    "import time, os, sys, numpy as np, pandas as pd\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# Ensure Spark exists and uses this Python\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    py = sys.executable\n",
    "    os.environ[\"PYSPARK_DRIVER_PYTHON\"] = py\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = py\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"Deterministic-Groupby\")\n",
    "        .config(\"spark.pyspark.driver.python\", py)\n",
    "        .config(\"spark.pyspark.python\", py)\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "# Params\n",
    "N = 100_000_000\n",
    "K = 1_000\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 16)\n",
    "\n",
    "# ---------- Spark: compute using integer sums, then convert ----------\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "df_s = (\n",
    "    spark.range(N)  # id: bigint\n",
    "    .withColumn(\"key\", (F.col(\"id\") % K).cast(\"int\"))\n",
    "    # keep integer id for exact sums; we'll divide by 2.0 later\n",
    "    .withColumn(\"id_int\", F.col(\"id\").cast(\"long\"))\n",
    ")\n",
    "\n",
    "lookup_s = spark.range(K).withColumnRenamed(\"id\", \"key\") \\\n",
    "    .withColumn(\"weight\", (F.col(\"key\") % 5).cast(\"int\"))\n",
    "\n",
    "joined_s = df_s.join(lookup_s, \"key\", \"inner\")\n",
    "\n",
    "agg_s_int = (\n",
    "    joined_s.groupBy(\"key\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"cnt\"),\n",
    "        F.sum(\"id_int\").alias(\"sum_id\"),\n",
    "        F.avg(\"weight\").alias(\"avg_weight\")\n",
    "    )\n",
    "    # Deterministic ordering: first by cnt desc, then by key asc\n",
    "    .orderBy(F.desc(\"cnt\"), F.asc(\"key\"))\n",
    ")\n",
    "\n",
    "# convert to final metrics\n",
    "agg_s = (\n",
    "    agg_s_int\n",
    "    .withColumn(\"sum_val\", F.col(\"sum_id\") * F.lit(0.5))     # exact then scaled\n",
    "    .withColumn(\"avg_val\", (F.col(\"sum_id\") * F.lit(0.5)) / F.col(\"cnt\"))\n",
    "    .select(\"key\", \"cnt\", \"sum_val\", \"avg_val\", \"avg_weight\")\n",
    ")\n",
    "\n",
    "spark_top5 = agg_s.limit(5).toPandas()\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "print(\"‚úÖ Spark (deterministic, stable) top-5:\")\n",
    "print(spark_top5)\n",
    "print(f\"‚è±Ô∏è Spark time: {t1 - t0:.2f} sec\\n\")\n",
    "\n",
    "# ---------- Pandas: same approach (integer sums, deterministic sort) ----------\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "ids = np.arange(N, dtype=np.int64)\n",
    "keys = (ids % K).astype(np.int32)\n",
    "id_int = ids  # exact\n",
    "\n",
    "df_p = pd.DataFrame({\"key\": keys, \"id_int\": id_int})\n",
    "lookup_p = pd.DataFrame({\"key\": np.arange(K, dtype=np.int32), \"weight\": np.arange(K, dtype=np.int32) % 5})\n",
    "\n",
    "joined_p = df_p.merge(lookup_p, on=\"key\", how=\"inner\")\n",
    "\n",
    "agg_p_int = (\n",
    "    joined_p.groupby(\"key\", sort=False)\n",
    "    .agg(cnt=(\"key\", \"size\"), sum_id=(\"id_int\", \"sum\"), avg_weight=(\"weight\", \"mean\"))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "agg_p_int[\"sum_val\"] = agg_p_int[\"sum_id\"] * 0.5\n",
    "agg_p_int[\"avg_val\"] = agg_p_int[\"sum_val\"] / agg_p_int[\"cnt\"]\n",
    "\n",
    "agg_p = agg_p_int.sort_values([\"cnt\", \"key\"], ascending=[False, True])[[\"key\",\"cnt\",\"sum_val\",\"avg_val\",\"avg_weight\"]]\n",
    "pandas_top5 = agg_p.head(5).reset_index(drop=True)\n",
    "\n",
    "t3 = time.perf_counter()\n",
    "\n",
    "print(\"‚úÖ Pandas (deterministic, stable) top-5:\")\n",
    "print(pandas_top5)\n",
    "print(f\"‚è±Ô∏è Pandas time: {t3 - t2:.2f} sec\\n\")\n",
    "\n",
    "# Quick equality check (within tolerance for floats)\n",
    "print(\"Columns equal (keys, cnt):\", \n",
    "      np.array_equal(spark_top5[\"key\"].to_numpy(), pandas_top5[\"key\"].to_numpy()) and \n",
    "      np.array_equal(spark_top5[\"cnt\"].to_numpy(), pandas_top5[\"cnt\"].to_numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d65d75",
   "metadata": {},
   "source": [
    "# üéØ Key Spark Interview Questions & Answers\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùì Why is Spark faster than Hadoop MapReduce?\n",
    "\n",
    "- **In-memory processing**: Spark keeps intermediate results in memory (RAM) instead of writing to disk between each map and reduce step (MapReduce uses disk I/O heavily).\n",
    "- **DAG Execution**: Spark uses a Directed Acyclic Graph (DAG) to optimize the entire job before execution, minimizing unnecessary data shuffles and recomputations.\n",
    "- **Optimized engine**: Spark uses pipelining and task scheduling that maximizes CPU utilization across cluster nodes.\n",
    "\n",
    "**Result:** Spark can be **10x‚Äì100x faster** than MapReduce for iterative or interactive workloads.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùì What does \"in-memory computation\" mean?\n",
    "\n",
    "- Intermediate data from transformations (like `map`, `filter`, `join`) are stored in **RAM** rather than on disk.\n",
    "- This avoids expensive read/write operations to disk.\n",
    "- Spark allows explicit **caching/persisting** of data in memory (`df.cache()`), which speeds up repeated operations on the same dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùì What are the main benefits of using Spark in a big data pipeline?\n",
    "\n",
    "1. **Speed** ‚Äì In-memory processing + DAG optimization = fast analytics.\n",
    "2. **Ease of Use** ‚Äì High-level APIs in Python, Scala, Java, SQL.\n",
    "3. **Unified Engine** ‚Äì Batch, streaming, ML, graph processing all in one framework.\n",
    "4. **Scalability** ‚Äì Runs locally or on clusters with thousands of nodes.\n",
    "5. **Fault Tolerance** ‚Äì Automatic recovery from failures using RDD lineage.\n",
    "6. **Wide Data Source Support** ‚Äì Reads/writes from HDFS, S3, JDBC, Kafka, Parquet, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùì How does Spark achieve fault tolerance?\n",
    "\n",
    "- Spark's core abstraction, **RDD (Resilient Distributed Dataset)**, keeps track of the **lineage** of transformations used to create it.\n",
    "- If a partition is lost (e.g., executor crashes), Spark **recomputes only the lost partitions** from their lineage rather than restarting the entire job.\n",
    "- For cached/persisted RDDs, Spark can replicate partitions across nodes (optional) to avoid recomputation.\n",
    "\n",
    "**In short:** Spark uses **lineage + recomputation** (and optionally replication) to handle node failures gracefully.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
