{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aafca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Point PySpark to the exact Python you're running right now ---\n",
    "import os, sys\n",
    "\n",
    "py = sys.executable  # e.g., C:\\Users\\PX\\anaconda3\\envs\\music-chatbot\\python.exe\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = py\n",
    "os.environ[\"PYSPARK_PYTHON\"] = py\n",
    "\n",
    "# --- 2) Stop any existing Spark session cleanly (if already created) ---\n",
    "try:\n",
    "    spark.stop()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"PySpark-Windows-Fix\")\n",
    "    # Ensures executors pick the same Python as the driver\n",
    "    .config(\"spark.pyspark.driver.python\", py)\n",
    "    .config(\"spark.pyspark.python\", py)\n",
    "    # Optional: Arrow speeds up Pandas â†” Spark conversions if available\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3397dd",
   "metadata": {},
   "source": [
    "# ðŸ”¥ PySpark Must-Know Commands (Cheat Sheet)\n",
    "\n",
    "Use this as your daily reference + interview primer. Each snippet shows what it does and why youâ€™d use it.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Start / Get SparkSession\n",
    "Create or get the main entry point to Spark SQL/DataFrame API:\n",
    "\n",
    "from pyspark.sql import SparkSession  \n",
    "spark = SparkSession.builder.appName(\"CheatSheet\").getOrCreate()\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Read & Write Data (CSV / Parquet / JSON)\n",
    "CSV: quick exploration  \n",
    "Parquet: columnar, compressed, fastest in Spark  \n",
    "JSON: semi-structured\n",
    "\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"data/file.csv\")  \n",
    "df.write.mode(\"overwrite\").option(\"header\", True).csv(\"out/csv/\")  \n",
    "dfp = spark.read.parquet(\"data/parquet/\")  \n",
    "dfp.write.mode(\"overwrite\").parquet(\"out/parquet/\")  \n",
    "dfj = spark.read.json(\"data/json/\")\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Inspect Data Quickly\n",
    "Peek rows, schema, columns, summary stats\n",
    "\n",
    "df.show(10, truncate=False)  \n",
    "df.printSchema()  \n",
    "df.columns  \n",
    "df.describe().show()\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Select, Filter, Where\n",
    "Core row/column selection and predicates\n",
    "\n",
    "from pyspark.sql import functions as F, types as T  \n",
    "df.select(\"colA\", \"colB\")  \n",
    "df.select(F.col(\"colA\").alias(\"a\"))  \n",
    "df.filter(F.col(\"amount\") > 100)  \n",
    "df.where((F.col(\"age\") >= 18) & (F.col(\"country\") == \"CH\"))\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Add / Transform Columns\n",
    "Create, cast, conditional logic, rename, drop\n",
    "\n",
    "df.withColumn(\"double_amt\", F.col(\"amount\") * 2)  \n",
    "df.withColumn(\"age_int\", F.col(\"age\").cast(T.IntegerType()))  \n",
    "df.withColumn(\"segment\", F.when(F.col(\"amount\") >= 1000, \"VIP\").otherwise(\"STD\"))  \n",
    "df.withColumnRenamed(\"old\", \"new\")  \n",
    "df.drop(\"unneeded_col\")\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Null Handling\n",
    "Drop/fill nulls; coalesce picks first non-null\n",
    "\n",
    "df.na.drop(subset=[\"age\", \"city\"])  \n",
    "df.na.fill({\"age\": 0, \"city\": \"Unknown\"})  \n",
    "df.select(F.coalesce(\"nickname\", \"name\").alias(\"display_name\"))\n",
    "\n",
    "---\n",
    "\n",
    "## 7) String & Date Helpers\n",
    "Common text transforms; parse and format dates/timestamps\n",
    "\n",
    "df.select(F.trim(\"name\"), F.lower(\"email\").alias(\"email_lc\"), F.concat_ws(\" \", \"first\", \"last\").alias(\"full_name\"))  \n",
    "df.select(F.to_date(\"dt\", \"yyyy-MM-dd\").alias(\"date\"), F.date_add(F.col(\"date\"), 7).alias(\"date_plus_7\"))\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Sort, Limit, Distinct\n",
    "Presentation + deduplication\n",
    "\n",
    "df.orderBy(F.desc(\"amount\"))  \n",
    "df.limit(100)  \n",
    "df.select(\"user_id\").distinct()  \n",
    "df.dropDuplicates([\"user_id\", \"day\"])\n",
    "\n",
    "---\n",
    "\n",
    "## 9) GroupBy & Aggregations\n",
    "Summaries by key(s); alias for readable column names\n",
    "\n",
    "agg = (df.groupBy(\"country\", \"year\")  \n",
    "        .agg(F.count(\"*\").alias(\"cnt\"),  \n",
    "             F.sum(\"amount\").alias(\"sum_amt\"),  \n",
    "             F.avg(\"amount\").alias(\"avg_amt\"),  \n",
    "             F.approx_count_distinct(\"user_id\").alias(\"approx_users\")))\n",
    "\n",
    "---\n",
    "\n",
    "## 10) Joins (inner/left/right/full/semi/anti)\n",
    "Combine datasets; semi/anti for existence filtering\n",
    "\n",
    "df1.join(df2, on=\"id\", how=\"inner\")  \n",
    "df1.join(df2, df1.id == df2.user_id, \"left\")  \n",
    "df_left_only = df1.join(df2, \"id\", \"left_anti\")  \n",
    "df_semi_exist = df1.join(df2, \"id\", \"left_semi\")\n",
    "\n",
    "---\n",
    "\n",
    "## 11) Window Functions (analytics over partitions)\n",
    "Rank, running totals, lag/lead within groups\n",
    "\n",
    "from pyspark.sql.window import Window  \n",
    "w = Window.partitionBy(\"country\").orderBy(F.desc(\"amount\"))  \n",
    "df.select(\"user\",\"country\",\"amount\",  \n",
    "          F.row_number().over(w).alias(\"rn\"),  \n",
    "          F.sum(\"amount\").over(w).alias(\"running_sum\"))\n",
    "\n",
    "---\n",
    "\n",
    "## 12) Repartition / Coalesce\n",
    "Control parallelism; coalesce only reduces partitions (no full shuffle)\n",
    "\n",
    "df = df.repartition(32, \"country\")  \n",
    "df_small = df.coalesce(4)\n",
    "\n",
    "---\n",
    "\n",
    "## 13) Cache / Persist / Unpersist\n",
    "Keep reused results in memory/disk to speed re-use\n",
    "\n",
    "df_cached = df.cache()  \n",
    "df_cached.count()  \n",
    "df_cached.unpersist()\n",
    "\n",
    "---\n",
    "\n",
    "## 14) SQL with Temp Views\n",
    "Mix SQL with DataFrame API; handy for quick queries\n",
    "\n",
    "df.createOrReplaceTempView(\"t\")  \n",
    "spark.sql(\"SELECT country, COUNT(*) cnt FROM t GROUP BY country ORDER BY cnt DESC\").show()\n",
    "\n",
    "---\n",
    "\n",
    "## 15) Write Options (mode, partitioning)\n",
    "Overwrite/append; partition files by columns for faster reads\n",
    "\n",
    "(df.write.mode(\"overwrite\")  \n",
    "    .partitionBy(\"year\",\"month\")  \n",
    "    .parquet(\"out/sales_by_ym/\"))\n",
    "\n",
    "---\n",
    "\n",
    "## 16) Read Multiple Files / Globs\n",
    "Ingest folders or patterns at once\n",
    "\n",
    "spark.read.parquet(\"s3://bucket/path/2025/*/*.parquet\")  \n",
    "spark.read.csv([\"path/a.csv\",\"path/b.csv\"], header=True)\n",
    "\n",
    "---\n",
    "\n",
    "## 17) Hints & Skew Helpers\n",
    "Broadcast for small dimension tables; salting/repartition for skew\n",
    "\n",
    "df_big.join(F.broadcast(df_small_dim), \"key\", \"inner\")  \n",
    "df_big.repartition(\"key\")\n",
    "\n",
    "---\n",
    "\n",
    "## 18) UDFs (use sparingly) & Built-ins\n",
    "Prefer built-ins; UDFs break optimizations. Pandas UDFs for vectorized speed.\n",
    "\n",
    "from pyspark.sql.functions import udf, pandas_udf  \n",
    "@udf(T.IntegerType())  \n",
    "def add1(x): return x + 1  \n",
    "df.withColumn(\"x_plus1\", add1(\"x\"))\n",
    "\n",
    "---\n",
    "\n",
    "## 19) Explain Plan\n",
    "See logical/physical plan; confirm shuffles and scans\n",
    "\n",
    "df.explain(mode=\"formatted\")\n",
    "\n",
    "---\n",
    "\n",
    "## 20) Pandas â†” Spark Interop\n",
    "Convert small results to Pandas; create Spark DF from Pandas\n",
    "\n",
    "pdf = df.limit(10000).toPandas()  \n",
    "df2 = spark.createDataFrame(pdf)\n",
    "\n",
    "---\n",
    "\n",
    "## 21) Sampling, Splits, Set Ops\n",
    "Quick samples; train/test split; set operations\n",
    "\n",
    "df.sample(withReplacement=False, fraction=0.1, seed=42)  \n",
    "train, test = df.randomSplit([0.8, 0.2], seed=42)  \n",
    "df1.unionByName(df2, allowMissingColumns=True)  \n",
    "df1.intersect(df2)  \n",
    "df1.subtract(df2)\n",
    "\n",
    "---\n",
    "\n",
    "## 22) Arrays / Maps / Explode\n",
    "Work with nested/list data\n",
    "\n",
    "df.select(F.size(\"items\"), F.array_contains(\"items\",\"foo\"))  \n",
    "df.select(F.explode(\"items\").alias(\"item\"))\n",
    "\n",
    "---\n",
    "\n",
    "## 23) JSON in Columns\n",
    "Parse JSON strings into structs; re-serialize when needed\n",
    "\n",
    "from pyspark.sql import types as T  \n",
    "schema = T.StructType([T.StructField(\"a\", T.IntegerType()), T.StructField(\"b\", T.StringType())])  \n",
    "df.select(F.from_json(\"json_str\", schema).alias(\"obj\")).select(\"obj.*\")\n",
    "\n",
    "---\n",
    "\n",
    "## 24) JDBC (Databases)\n",
    "Read/write via JDBC connectors (supply driver JAR)\n",
    "\n",
    "jdbc_df = (spark.read.format(\"jdbc\")  \n",
    "    .option(\"url\", \"jdbc:postgresql://host/db\")  \n",
    "    .option(\"dbtable\", \"public.sales\")  \n",
    "    .option(\"user\", \"user\").option(\"password\", \"pass\")  \n",
    "    .load())\n",
    "\n",
    "---\n",
    "\n",
    "## 25) Write Modes Recap\n",
    "Control overwrite/append behavior\n",
    "\n",
    "df.write.mode(\"overwrite\").parquet(\"out/\")\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Pro Tips\n",
    "- Prefer Parquet + column pruning + predicate pushdown  \n",
    "- Keep transformations narrow before wide ops (filter early)  \n",
    "- Tune spark.sql.shuffle.partitions to match cluster size  \n",
    "- Use broadcast joins for small dims, cache reused DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c572b8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists: c:\\Users\\PX\\Desktop\\java-python-ipynb-practice\\Python\\3. Pyspark\\data\\titanic.csv\n",
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| NULL|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| NULL|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import urllib.request\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1) Pick a CSV to download (replace with any direct CSV URL if you want)\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "data_dir = Path.cwd() / \"data\"\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "local_csv = data_dir / \"titanic.csv\"\n",
    "\n",
    "# 2) Download to local file (only if not already present)\n",
    "if not local_csv.exists():\n",
    "    print(f\"Downloading to {local_csv} ...\")\n",
    "    urllib.request.urlretrieve(url, local_csv.as_posix())\n",
    "else:\n",
    "    print(f\"Already exists: {local_csv}\")\n",
    "\n",
    "# 3) Read with Spark\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(local_csv.as_posix())\n",
    "\n",
    "# 4) Inspect\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79e9800f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Filtered adults (Age >= 18) â€“ sample ===\n",
      "+-----------+---------------------------------------------------+------+----+------+--------+\n",
      "|PassengerId|Name                                               |Sex   |Age |Pclass|Survived|\n",
      "+-----------+---------------------------------------------------+------+----+------+--------+\n",
      "|1          |Braund, Mr. Owen Harris                            |male  |22.0|3     |0       |\n",
      "|2          |Cumings, Mrs. John Bradley (Florence Briggs Thayer)|female|38.0|1     |1       |\n",
      "|3          |Heikkinen, Miss. Laina                             |female|26.0|3     |1       |\n",
      "|4          |Futrelle, Mrs. Jacques Heath (Lily May Peel)       |female|35.0|1     |1       |\n",
      "|5          |Allen, Mr. William Henry                           |male  |35.0|3     |0       |\n",
      "+-----------+---------------------------------------------------+------+----+------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "=== Aggregation by Sex, Pclass ===\n",
      "+------+------+---+------------------+-------------------+\n",
      "|Sex   |Pclass|cnt|avg_age           |survival_rate      |\n",
      "+------+------+---+------------------+-------------------+\n",
      "|male  |3     |210|29.895238095238096|0.13333333333333333|\n",
      "|male  |1     |97 |42.644329896907216|0.3711340206185567 |\n",
      "|male  |2     |88 |33.98863636363637 |0.06818181818181818|\n",
      "|female|1     |77 |36.74025974025974 |0.974025974025974  |\n",
      "|female|3     |67 |28.708955223880597|0.417910447761194  |\n",
      "|female|2     |62 |32.66935483870968 |0.9032258064516129 |\n",
      "+------+------+---+------------------+-------------------+\n",
      "\n",
      "\n",
      "=== Null handling (filled Age with 0) â€“ sample ===\n",
      "+-----------+---------------------------------------------------+------+----+------+--------+\n",
      "|PassengerId|Name                                               |Sex   |Age |Pclass|Survived|\n",
      "+-----------+---------------------------------------------------+------+----+------+--------+\n",
      "|1          |Braund, Mr. Owen Harris                            |male  |22.0|3     |0       |\n",
      "|2          |Cumings, Mrs. John Bradley (Florence Briggs Thayer)|female|38.0|1     |1       |\n",
      "|3          |Heikkinen, Miss. Laina                             |female|26.0|3     |1       |\n",
      "|4          |Futrelle, Mrs. Jacques Heath (Lily May Peel)       |female|35.0|1     |1       |\n",
      "|5          |Allen, Mr. William Henry                           |male  |35.0|3     |0       |\n",
      "+-----------+---------------------------------------------------+------+----+------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Local CSV path used: c:/Users/PX/Desktop/java-python-ipynb-practice/Python/3. Pyspark/data/titanic.csv\n"
     ]
    }
   ],
   "source": [
    "# Select & Filter\n",
    "df_select = df.select(\"PassengerId\", \"Name\", \"Sex\", \"Age\", \"Pclass\", \"Survived\")\n",
    "df_filter = df_select.filter(F.col(\"Age\") >= 18)\n",
    "\n",
    "# GroupBy & Aggregation\n",
    "agg = (\n",
    "    df_filter.groupBy(\"Sex\", \"Pclass\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"cnt\"),\n",
    "        F.avg(\"Age\").alias(\"avg_age\"),\n",
    "        F.avg(\"Survived\").alias(\"survival_rate\")\n",
    "    )\n",
    "    .orderBy(F.desc(\"cnt\"))\n",
    ")\n",
    "\n",
    "# Null handling\n",
    "clean = df_select.na.fill({\"Age\": 0})\n",
    "\n",
    "# Show results\n",
    "print(\"\\n=== Filtered adults (Age >= 18) â€“ sample ===\")\n",
    "df_filter.show(5, truncate=False)\n",
    "\n",
    "print(\"\\n=== Aggregation by Sex, Pclass ===\")\n",
    "agg.show(10, truncate=False)\n",
    "\n",
    "print(\"\\n=== Null handling (filled Age with 0) â€“ sample ===\")\n",
    "clean.show(5, truncate=False)\n",
    "\n",
    "print(f\"\\nLocal CSV path used: {local_csv.as_posix()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
